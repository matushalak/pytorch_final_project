{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3402977c-4b43-4d9e-9e4c-970e0b6826b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all modules imported!\n"
     ]
    }
   ],
   "source": [
    "# Pytorch modules\n",
    "import torch\n",
    "# main class used for making neural networks\n",
    "from torch import nn\n",
    "# pytorch has all the datasets I need already in its computer vision module\n",
    "from torchvision import datasets\n",
    "# used to parallelize loading (my laptop has only 4 cores)\n",
    "from torch.utils.data import DataLoader\n",
    "# used to transform input data into desired format\n",
    "from torchvision import transforms\n",
    "\n",
    "# general python modules\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print('all modules imported!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bf0de53-7756-41ac-b9bf-313317bb934f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip to .\\EMNIST\\raw\\gzip.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████| 561753746/561753746 [38:42<00:00, 241914.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting .\\EMNIST\\raw\\gzip.zip to .\\EMNIST\\raw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([112800, 28, 28])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define transformations we will apply to images before using them\n",
    "transform = transforms.Compose([\n",
    "    # same size all (MNIST images 28x28)\n",
    "    transforms.Resize([28,28]),\n",
    "    # transform all to greyscale\n",
    "    transforms.Grayscale(),\n",
    "    # transform to tensor representation\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# get the EMNIST (letters and digits) data and apply our transform to it and save to same directory\n",
    "# train = True by default\n",
    "emnist = datasets.EMNIST(root='.', # same directory\n",
    "                         split = 'balanced', #balanced number of labels for each category \n",
    "                       download=True, # if already downloaded won't do anything\n",
    "                       transform = transform)\n",
    "emnist.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "721eeffd-7fab-4341-86dd-8614abdf0610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__add__', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_all_classes', '_check_exists', '_check_legacy_exist', '_file_prefix', '_format_transform_repr', '_is_protocol', '_load_data', '_load_legacy_data', '_merged_classes', '_repr_indent', '_test_file', '_training_file', 'class_to_idx', 'classes', 'classes_split_dict', 'data', 'download', 'extra_repr', 'images_file', 'labels_file', 'md5', 'mirrors', 'processed_folder', 'raw_folder', 'resources', 'root', 'split', 'splits', 'target_transform', 'targets', 'test_data', 'test_file', 'test_labels', 'train', 'train_data', 'train_labels', 'training_file', 'transform', 'transforms', 'url']\n",
      "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, 'A': 10, 'B': 11, 'C': 12, 'D': 13, 'E': 14, 'F': 15, 'G': 16, 'H': 17, 'I': 18, 'J': 19, 'K': 20, 'L': 21, 'M': 22, 'N': 23, 'O': 24, 'P': 25, 'Q': 26, 'R': 27, 'S': 28, 'T': 29, 'U': 30, 'V': 31, 'W': 32, 'X': 33, 'Y': 34, 'Z': 35, 'a': 36, 'b': 37, 'd': 38, 'e': 39, 'f': 40, 'g': 41, 'h': 42, 'n': 43, 'q': 44, 'r': 45, 't': 46}\n"
     ]
    }
   ],
   "source": [
    "print(dir(emnist))\n",
    "\n",
    "print(emnist.class_to_idx) # only letters which look different as uppercase/lowercase have separate label for lowercase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
