{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda501c6-01a7-4f05-8d15-b82f16912349",
   "metadata": {},
   "source": [
    "Matus Halak Data Processing Final Project (pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280eaea8-640e-45b5-8b3d-b9af913a5627",
   "metadata": {},
   "source": [
    "Start off easy: MNIST digit classification\n",
    "MNIST dataset\n",
    "70,000 labeled images of handdrawn digits 0-9 using Fully-connected Multi-layer Perceptron\n",
    "at the end, try to classify own handwritten images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e832cccd-b356-4627-8d95-5a7b7d601972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all modules imported!\n"
     ]
    }
   ],
   "source": [
    "# Pytorch modules\n",
    "import torch\n",
    "# main class used for making neural networks\n",
    "from torch import nn\n",
    "# pytorch has all the datasets I need already in its computer vision module\n",
    "from torchvision import datasets\n",
    "# used to parallelize loading (my laptop has only 4 cores)\n",
    "from torch.utils.data import DataLoader\n",
    "# used to transform input data into desired format\n",
    "from torchvision import transforms\n",
    "\n",
    "# general python modules\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print('all modules imported!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5a6f6e4-069f-47de-b74f-50f7d8bec896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define transformations we will apply to images before using them\n",
    "transform = transforms.Compose([\n",
    "    # same size all (MNIST images 28x28)\n",
    "    transforms.Resize([28,28]),\n",
    "    # transform to tensor representation\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# get the MNIST data and apply our transform to it and save to same directory\n",
    "# train = True by default\n",
    "mnist = datasets.MNIST(root='.', # same directory\n",
    "                       download=True, # if already downloaded won't do anything\n",
    "                       transform = transform)\n",
    "# normalize\n",
    "mnist.data = mnist.data / 255\n",
    "\n",
    "mnist.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fc028a9-b4ed-44f8-a04b-1da1564dd911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One batch: torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# batch size seems to be a parameter that one can play with a lot\n",
    "# will try different ones (apparently smaller batch size = faster training but more noise)\n",
    "\n",
    "# iterator of the dataset (always returns the n = batch size of \n",
    "# images with labels in each iteration)\n",
    "dataloader = DataLoader(dataset = mnist, \n",
    "                        batch_size = 64,\n",
    "                        num_workers = 4) # 4 cpu cores on my laptop\n",
    "\n",
    "image, label = next(iter(dataloader))\n",
    "\n",
    "print('One batch:', image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6466b44f-20e5-4827-aaf5-0db4f4b3d12c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy.random import randint\n",
    "fig, axes = plt.subplots(3,3,figsize = (5,5))\n",
    "axes = axes.flatten()\n",
    "for ax, image_eg in enumerate(randint(0,64,size = 9)):\n",
    "    axes[ax].set_title(f'Label: {label[image_eg]}')\n",
    "    axes[ax].axis('off')\n",
    "    axes[ax].imshow(transforms.ToPILImage()(image[image_eg]))\n",
    "# Example data\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcf9739-704f-4916-a625-34847fdbc943",
   "metadata": {},
   "source": [
    "Building a NN\n",
    "-  input layer: 28x28 = 784 inputs(image size) (value 0-1, normalized)\n",
    "-  hidden layer(s): hard to determine parameters, need to play around\n",
    "    - how many layers ?\n",
    "    - how many nodes ? - more than input layer?\n",
    "- output layer: 10 outputs (digits 0-9) (value 0-1; the output with the highest value is the predicted label)\n",
    "- weights between nodes of different layers (value 0-1; initiallize randomly)\n",
    "- each node in hidden & output layers performs a weighed sum (or some other operation) on the node 'output' values and weights from previous layer\n",
    "- an activation function and bias determines how this 'weighed sum' will be reflected in the 'output' of the current node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "268f4af5-c5d1-4d83-a3a6-16d50c90bd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My_First_NN(\n",
      "  (layers): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=784, out_features=200, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=200, out_features=300, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=300, out_features=200, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (3): Linear(in_features=200, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# fully connected (fc) block (will repeat for multiple hidden layers)\n",
    "def fc_block(input_size:int,\n",
    "            output_size:int,\n",
    "            activation_function:object):\n",
    "    return nn.Sequential(\n",
    "        # how to perform \"weighed sum\" on 'output' of previous layer\n",
    "        nn.Linear(input_size,output_size),\n",
    "        # what to do with \"weighed sum\"\n",
    "        activation_function())\n",
    "    \n",
    "# start with fully connected network\n",
    "class My_First_NN (nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels:int,\n",
    "                 hidden_layers_sizes:list,\n",
    "                 out_channels:int,\n",
    "                activation:object):\n",
    "        # initialize whatever is in __init__ of nn.Module\n",
    "        super(My_First_NN, self).__init__()\n",
    "        # this one way how input & hidden layers can be defined\n",
    "        # sizes (number of nodes) of each layer\n",
    "        layer_sizes = [in_channels,*hidden_layers_sizes]\n",
    "        # make blocks of fully connected layers \n",
    "        fully_connected_blocks = [fc_block(in_n, out_n, activation)\n",
    "                                  for in_n, out_n in zip(layer_sizes, layer_sizes[1:])]\n",
    "        # these will be sequentially executed in the forward pass\n",
    "        self.layers = nn.Sequential(*fully_connected_blocks,\n",
    "                                    # for output layer only want weighed sum\n",
    "                                    nn.Linear(layer_sizes[-1],out_channels))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # flatten input\n",
    "        x = x.view(x.size(0),-1)\n",
    "        # forward pass of input through network\n",
    "        x = self.layers(x)\n",
    "        # at the end, x is a list of 10 numbers from 0-10 telling us\n",
    "        # which label the model predicts\n",
    "        return x\n",
    "\n",
    "first_model = My_First_NN(784,\n",
    "                          # completely arbitrary number of hidden layers & nodes\n",
    "                         [200,300,200],\n",
    "                         10,\n",
    "                         nn.ReLU)\n",
    "# inspect model architecture\n",
    "print(first_model) # yes, as intended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b6b971-30bc-488f-ac88-61f3222fbf70",
   "metadata": {},
   "source": [
    "Training:\n",
    "- after one forward pass, get distribution of numbers in output layer which reflects \"probability\" that a given number is on the screen (highest output = highest probability that it is that label)\n",
    "- we need to quantify how different the distribution is from the distribution that we want for the given label (0 prob for 9 digits not in picture and 1 prob for 1 digit on the picture) = Error\n",
    "- calculate the gradient given the error\n",
    "- update the weights according to the gradient (take a step down the gradient proportional to learning rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2c258da-8595-42d0-801e-851af008f0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 done\n",
      "0 1 done\n",
      "0 2 done\n",
      "0 3 done\n",
      "0 4 done\n",
      "0 5 done\n",
      "0 6 done\n",
      "0 7 done\n",
      "0 8 done\n",
      "0 9 done\n",
      "0 10 done\n",
      "0 11 done\n",
      "0 12 done\n",
      "0 13 done\n",
      "0 14 done\n",
      "0 15 done\n",
      "0 16 done\n",
      "0 17 done\n",
      "0 18 done\n",
      "0 19 done\n",
      "0 20 done\n",
      "0 21 done\n",
      "0 22 done\n",
      "0 23 done\n",
      "0 24 done\n",
      "0 25 done\n",
      "0 26 done\n",
      "0 27 done\n",
      "0 28 done\n",
      "0 29 done\n",
      "0 30 done\n",
      "0 31 done\n",
      "0 32 done\n",
      "0 33 done\n",
      "0 34 done\n",
      "0 35 done\n",
      "0 36 done\n",
      "0 37 done\n",
      "0 38 done\n",
      "0 39 done\n",
      "0 40 done\n",
      "0 41 done\n",
      "0 42 done\n",
      "0 43 done\n",
      "0 44 done\n",
      "0 45 done\n",
      "0 46 done\n",
      "0 47 done\n",
      "0 48 done\n",
      "0 49 done\n",
      "0 50 done\n",
      "0 51 done\n",
      "0 52 done\n",
      "0 53 done\n",
      "0 54 done\n",
      "0 55 done\n",
      "0 56 done\n",
      "0 57 done\n",
      "0 58 done\n",
      "0 59 done\n",
      "0 60 done\n",
      "0 61 done\n",
      "0 62 done\n",
      "0 63 done\n",
      "0 64 done\n",
      "0 65 done\n",
      "0 66 done\n",
      "0 67 done\n",
      "0 68 done\n",
      "0 69 done\n",
      "0 70 done\n",
      "0 71 done\n",
      "0 72 done\n",
      "0 73 done\n",
      "0 74 done\n",
      "0 75 done\n",
      "0 76 done\n",
      "0 77 done\n",
      "0 78 done\n",
      "0 79 done\n",
      "0 80 done\n",
      "0 81 done\n",
      "0 82 done\n",
      "0 83 done\n",
      "0 84 done\n",
      "0 85 done\n",
      "0 86 done\n",
      "0 87 done\n",
      "0 88 done\n",
      "0 89 done\n",
      "0 90 done\n",
      "0 91 done\n",
      "0 92 done\n",
      "0 93 done\n",
      "0 94 done\n",
      "0 95 done\n",
      "0 96 done\n",
      "0 97 done\n",
      "0 98 done\n",
      "0 99 done\n",
      "0 100 done\n",
      "0 101 done\n",
      "0 102 done\n",
      "0 103 done\n",
      "0 104 done\n",
      "0 105 done\n",
      "0 106 done\n",
      "0 107 done\n",
      "0 108 done\n",
      "0 109 done\n",
      "0 110 done\n",
      "0 111 done\n",
      "0 112 done\n",
      "0 113 done\n",
      "0 114 done\n",
      "0 115 done\n",
      "0 116 done\n",
      "0 117 done\n",
      "0 118 done\n",
      "0 119 done\n",
      "0 120 done\n",
      "0 121 done\n",
      "0 122 done\n",
      "0 123 done\n",
      "0 124 done\n",
      "0 125 done\n",
      "0 126 done\n",
      "0 127 done\n",
      "0 128 done\n",
      "0 129 done\n",
      "0 130 done\n",
      "0 131 done\n",
      "0 132 done\n",
      "0 133 done\n",
      "0 134 done\n",
      "0 135 done\n",
      "0 136 done\n",
      "0 137 done\n",
      "0 138 done\n",
      "0 139 done\n",
      "0 140 done\n",
      "0 141 done\n",
      "0 142 done\n",
      "0 143 done\n",
      "0 144 done\n",
      "0 145 done\n",
      "0 146 done\n",
      "0 147 done\n",
      "0 148 done\n",
      "0 149 done\n",
      "0 150 done\n",
      "0 151 done\n",
      "0 152 done\n",
      "0 153 done\n",
      "0 154 done\n",
      "0 155 done\n",
      "0 156 done\n",
      "0 157 done\n",
      "0 158 done\n",
      "0 159 done\n",
      "0 160 done\n",
      "0 161 done\n",
      "0 162 done\n",
      "0 163 done\n",
      "0 164 done\n",
      "0 165 done\n",
      "0 166 done\n",
      "0 167 done\n",
      "0 168 done\n",
      "0 169 done\n",
      "0 170 done\n",
      "0 171 done\n",
      "0 172 done\n",
      "0 173 done\n",
      "0 174 done\n",
      "0 175 done\n",
      "0 176 done\n",
      "0 177 done\n",
      "0 178 done\n",
      "0 179 done\n",
      "0 180 done\n",
      "0 181 done\n",
      "0 182 done\n",
      "0 183 done\n",
      "0 184 done\n",
      "0 185 done\n",
      "0 186 done\n",
      "0 187 done\n",
      "0 188 done\n",
      "0 189 done\n",
      "0 190 done\n",
      "0 191 done\n",
      "0 192 done\n",
      "0 193 done\n",
      "0 194 done\n",
      "0 195 done\n",
      "0 196 done\n",
      "0 197 done\n",
      "0 198 done\n",
      "0 199 done\n",
      "0 200 done\n",
      "0 201 done\n",
      "0 202 done\n",
      "0 203 done\n",
      "0 204 done\n",
      "0 205 done\n",
      "0 206 done\n",
      "0 207 done\n",
      "0 208 done\n",
      "0 209 done\n",
      "0 210 done\n",
      "0 211 done\n",
      "0 212 done\n",
      "0 213 done\n",
      "0 214 done\n",
      "0 215 done\n",
      "0 216 done\n",
      "0 217 done\n",
      "0 218 done\n",
      "0 219 done\n",
      "0 220 done\n",
      "0 221 done\n",
      "0 222 done\n",
      "0 223 done\n",
      "0 224 done\n",
      "0 225 done\n",
      "0 226 done\n",
      "0 227 done\n",
      "0 228 done\n",
      "0 229 done\n",
      "0 230 done\n",
      "0 231 done\n",
      "0 232 done\n",
      "0 233 done\n",
      "0 234 done\n",
      "0 235 done\n",
      "0 236 done\n",
      "0 237 done\n",
      "0 238 done\n",
      "0 239 done\n",
      "0 240 done\n",
      "0 241 done\n",
      "0 242 done\n",
      "0 243 done\n",
      "0 244 done\n",
      "0 245 done\n",
      "0 246 done\n",
      "0 247 done\n",
      "0 248 done\n",
      "0 249 done\n",
      "0 250 done\n",
      "0 251 done\n",
      "0 252 done\n",
      "0 253 done\n",
      "0 254 done\n",
      "0 255 done\n",
      "0 256 done\n",
      "0 257 done\n",
      "0 258 done\n",
      "0 259 done\n",
      "0 260 done\n",
      "0 261 done\n",
      "0 262 done\n",
      "0 263 done\n",
      "0 264 done\n",
      "0 265 done\n",
      "0 266 done\n",
      "0 267 done\n",
      "0 268 done\n",
      "0 269 done\n",
      "0 270 done\n",
      "0 271 done\n",
      "0 272 done\n",
      "0 273 done\n",
      "0 274 done\n",
      "0 275 done\n",
      "0 276 done\n",
      "0 277 done\n",
      "0 278 done\n",
      "0 279 done\n",
      "0 280 done\n",
      "0 281 done\n",
      "0 282 done\n",
      "0 283 done\n",
      "0 284 done\n",
      "0 285 done\n",
      "0 286 done\n",
      "0 287 done\n",
      "0 288 done\n",
      "0 289 done\n",
      "0 290 done\n",
      "0 291 done\n",
      "0 292 done\n",
      "0 293 done\n",
      "0 294 done\n",
      "0 295 done\n",
      "0 296 done\n",
      "0 297 done\n",
      "0 298 done\n",
      "0 299 done\n",
      "0 300 done\n",
      "0 301 done\n",
      "0 302 done\n",
      "0 303 done\n",
      "0 304 done\n",
      "0 305 done\n",
      "0 306 done\n",
      "0 307 done\n",
      "0 308 done\n",
      "0 309 done\n",
      "0 310 done\n",
      "0 311 done\n",
      "0 312 done\n",
      "0 313 done\n",
      "0 314 done\n",
      "0 315 done\n",
      "0 316 done\n",
      "0 317 done\n",
      "0 318 done\n",
      "0 319 done\n",
      "0 320 done\n",
      "0 321 done\n",
      "0 322 done\n",
      "0 323 done\n",
      "0 324 done\n",
      "0 325 done\n",
      "0 326 done\n",
      "0 327 done\n",
      "0 328 done\n",
      "0 329 done\n",
      "0 330 done\n",
      "0 331 done\n",
      "0 332 done\n",
      "0 333 done\n",
      "0 334 done\n",
      "0 335 done\n",
      "0 336 done\n",
      "0 337 done\n",
      "0 338 done\n",
      "0 339 done\n",
      "0 340 done\n",
      "0 341 done\n",
      "0 342 done\n",
      "0 343 done\n",
      "0 344 done\n",
      "0 345 done\n",
      "0 346 done\n",
      "0 347 done\n",
      "0 348 done\n",
      "0 349 done\n",
      "0 350 done\n",
      "0 351 done\n",
      "0 352 done\n",
      "0 353 done\n",
      "0 354 done\n",
      "0 355 done\n",
      "0 356 done\n",
      "0 357 done\n",
      "0 358 done\n",
      "0 359 done\n",
      "0 360 done\n",
      "0 361 done\n",
      "0 362 done\n",
      "0 363 done\n",
      "0 364 done\n",
      "0 365 done\n",
      "0 366 done\n",
      "0 367 done\n",
      "0 368 done\n",
      "0 369 done\n",
      "0 370 done\n",
      "0 371 done\n",
      "0 372 done\n",
      "0 373 done\n",
      "0 374 done\n",
      "0 375 done\n",
      "0 376 done\n",
      "0 377 done\n",
      "0 378 done\n",
      "0 379 done\n",
      "0 380 done\n",
      "0 381 done\n",
      "0 382 done\n",
      "0 383 done\n",
      "0 384 done\n",
      "0 385 done\n",
      "0 386 done\n",
      "0 387 done\n",
      "0 388 done\n",
      "0 389 done\n",
      "0 390 done\n",
      "0 391 done\n",
      "0 392 done\n",
      "0 393 done\n",
      "0 394 done\n",
      "0 395 done\n",
      "0 396 done\n",
      "0 397 done\n",
      "0 398 done\n",
      "0 399 done\n",
      "0 400 done\n",
      "0 401 done\n",
      "0 402 done\n",
      "0 403 done\n",
      "0 404 done\n",
      "0 405 done\n",
      "0 406 done\n",
      "0 407 done\n",
      "0 408 done\n",
      "0 409 done\n",
      "0 410 done\n",
      "0 411 done\n",
      "0 412 done\n",
      "0 413 done\n",
      "0 414 done\n",
      "0 415 done\n",
      "0 416 done\n",
      "0 417 done\n",
      "0 418 done\n",
      "0 419 done\n",
      "0 420 done\n",
      "0 421 done\n",
      "0 422 done\n",
      "0 423 done\n",
      "0 424 done\n",
      "0 425 done\n",
      "0 426 done\n",
      "0 427 done\n",
      "0 428 done\n",
      "0 429 done\n",
      "0 430 done\n",
      "0 431 done\n",
      "0 432 done\n",
      "0 433 done\n",
      "0 434 done\n",
      "0 435 done\n",
      "0 436 done\n",
      "0 437 done\n",
      "0 438 done\n",
      "0 439 done\n",
      "0 440 done\n",
      "0 441 done\n",
      "0 442 done\n",
      "0 443 done\n",
      "0 444 done\n",
      "0 445 done\n",
      "0 446 done\n",
      "0 447 done\n",
      "0 448 done\n",
      "0 449 done\n",
      "0 450 done\n",
      "0 451 done\n",
      "0 452 done\n",
      "0 453 done\n",
      "0 454 done\n",
      "0 455 done\n",
      "0 456 done\n",
      "0 457 done\n",
      "0 458 done\n",
      "0 459 done\n",
      "0 460 done\n",
      "0 461 done\n",
      "0 462 done\n",
      "0 463 done\n",
      "0 464 done\n",
      "0 465 done\n",
      "0 466 done\n",
      "0 467 done\n",
      "0 468 done\n",
      "0 469 done\n",
      "0 470 done\n",
      "0 471 done\n",
      "0 472 done\n",
      "0 473 done\n",
      "0 474 done\n",
      "0 475 done\n",
      "0 476 done\n",
      "0 477 done\n",
      "0 478 done\n",
      "0 479 done\n",
      "0 480 done\n",
      "0 481 done\n",
      "0 482 done\n",
      "0 483 done\n",
      "0 484 done\n",
      "0 485 done\n",
      "0 486 done\n",
      "0 487 done\n",
      "0 488 done\n",
      "0 489 done\n",
      "0 490 done\n",
      "0 491 done\n",
      "0 492 done\n",
      "0 493 done\n",
      "0 494 done\n",
      "0 495 done\n",
      "0 496 done\n",
      "0 497 done\n",
      "0 498 done\n",
      "0 499 done\n",
      "0 500 done\n",
      "0 501 done\n",
      "0 502 done\n",
      "0 503 done\n",
      "0 504 done\n",
      "0 505 done\n",
      "0 506 done\n",
      "0 507 done\n",
      "0 508 done\n",
      "0 509 done\n",
      "0 510 done\n",
      "0 511 done\n",
      "0 512 done\n",
      "0 513 done\n",
      "0 514 done\n",
      "0 515 done\n",
      "0 516 done\n",
      "0 517 done\n",
      "0 518 done\n",
      "0 519 done\n",
      "0 520 done\n",
      "0 521 done\n",
      "0 522 done\n",
      "0 523 done\n",
      "0 524 done\n",
      "0 525 done\n",
      "0 526 done\n",
      "0 527 done\n",
      "0 528 done\n",
      "0 529 done\n",
      "0 530 done\n",
      "0 531 done\n",
      "0 532 done\n",
      "0 533 done\n",
      "0 534 done\n",
      "0 535 done\n",
      "0 536 done\n",
      "0 537 done\n",
      "0 538 done\n",
      "0 539 done\n",
      "0 540 done\n",
      "0 541 done\n",
      "0 542 done\n",
      "0 543 done\n",
      "0 544 done\n",
      "0 545 done\n",
      "0 546 done\n",
      "0 547 done\n",
      "0 548 done\n",
      "0 549 done\n",
      "0 550 done\n",
      "0 551 done\n",
      "0 552 done\n",
      "0 553 done\n",
      "0 554 done\n",
      "0 555 done\n",
      "0 556 done\n",
      "0 557 done\n",
      "0 558 done\n",
      "0 559 done\n",
      "0 560 done\n",
      "0 561 done\n",
      "0 562 done\n",
      "0 563 done\n",
      "0 564 done\n",
      "0 565 done\n",
      "0 566 done\n",
      "0 567 done\n",
      "0 568 done\n",
      "0 569 done\n",
      "0 570 done\n",
      "0 571 done\n",
      "0 572 done\n",
      "0 573 done\n",
      "0 574 done\n",
      "0 575 done\n",
      "0 576 done\n",
      "0 577 done\n",
      "0 578 done\n",
      "0 579 done\n",
      "0 580 done\n",
      "0 581 done\n",
      "0 582 done\n",
      "0 583 done\n",
      "0 584 done\n",
      "0 585 done\n",
      "0 586 done\n",
      "0 587 done\n",
      "0 588 done\n",
      "0 589 done\n",
      "0 590 done\n",
      "0 591 done\n",
      "0 592 done\n",
      "0 593 done\n",
      "0 594 done\n",
      "0 595 done\n",
      "0 596 done\n",
      "0 597 done\n",
      "0 598 done\n",
      "0 599 done\n",
      "0 600 done\n",
      "0 601 done\n",
      "0 602 done\n",
      "0 603 done\n",
      "0 604 done\n",
      "0 605 done\n",
      "0 606 done\n",
      "0 607 done\n",
      "0 608 done\n",
      "0 609 done\n",
      "0 610 done\n",
      "0 611 done\n",
      "0 612 done\n",
      "0 613 done\n",
      "0 614 done\n",
      "0 615 done\n",
      "0 616 done\n",
      "0 617 done\n",
      "0 618 done\n",
      "0 619 done\n",
      "0 620 done\n",
      "0 621 done\n",
      "0 622 done\n",
      "0 623 done\n",
      "0 624 done\n",
      "0 625 done\n",
      "0 626 done\n",
      "0 627 done\n",
      "0 628 done\n",
      "0 629 done\n",
      "0 630 done\n",
      "0 631 done\n",
      "0 632 done\n",
      "0 633 done\n",
      "0 634 done\n",
      "0 635 done\n",
      "0 636 done\n",
      "0 637 done\n",
      "0 638 done\n",
      "0 639 done\n",
      "0 640 done\n",
      "0 641 done\n",
      "0 642 done\n",
      "0 643 done\n",
      "0 644 done\n",
      "0 645 done\n",
      "0 646 done\n",
      "0 647 done\n",
      "0 648 done\n",
      "0 649 done\n",
      "0 650 done\n",
      "0 651 done\n",
      "0 652 done\n",
      "0 653 done\n",
      "0 654 done\n",
      "0 655 done\n",
      "0 656 done\n",
      "0 657 done\n",
      "0 658 done\n",
      "0 659 done\n",
      "0 660 done\n",
      "0 661 done\n",
      "0 662 done\n",
      "0 663 done\n",
      "0 664 done\n",
      "0 665 done\n",
      "0 666 done\n",
      "0 667 done\n",
      "0 668 done\n",
      "0 669 done\n",
      "0 670 done\n",
      "0 671 done\n",
      "0 672 done\n",
      "0 673 done\n",
      "0 674 done\n",
      "0 675 done\n",
      "0 676 done\n",
      "0 677 done\n",
      "0 678 done\n",
      "0 679 done\n",
      "0 680 done\n",
      "0 681 done\n",
      "0 682 done\n",
      "0 683 done\n",
      "0 684 done\n",
      "0 685 done\n",
      "0 686 done\n",
      "0 687 done\n",
      "0 688 done\n",
      "0 689 done\n",
      "0 690 done\n",
      "0 691 done\n",
      "0 692 done\n",
      "0 693 done\n",
      "0 694 done\n",
      "0 695 done\n",
      "0 696 done\n",
      "0 697 done\n",
      "0 698 done\n",
      "0 699 done\n",
      "0 700 done\n",
      "0 701 done\n",
      "0 702 done\n",
      "0 703 done\n",
      "0 704 done\n",
      "0 705 done\n",
      "0 706 done\n",
      "0 707 done\n",
      "0 708 done\n",
      "0 709 done\n",
      "0 710 done\n",
      "0 711 done\n",
      "0 712 done\n",
      "0 713 done\n",
      "0 714 done\n",
      "0 715 done\n",
      "0 716 done\n",
      "0 717 done\n",
      "0 718 done\n",
      "0 719 done\n",
      "0 720 done\n",
      "0 721 done\n",
      "0 722 done\n",
      "0 723 done\n",
      "0 724 done\n",
      "0 725 done\n",
      "0 726 done\n",
      "0 727 done\n",
      "0 728 done\n",
      "0 729 done\n",
      "0 730 done\n",
      "0 731 done\n",
      "0 732 done\n",
      "0 733 done\n",
      "0 734 done\n",
      "0 735 done\n",
      "0 736 done\n",
      "0 737 done\n",
      "0 738 done\n",
      "0 739 done\n",
      "0 740 done\n",
      "0 741 done\n",
      "0 742 done\n",
      "0 743 done\n",
      "0 744 done\n",
      "0 745 done\n",
      "0 746 done\n",
      "0 747 done\n",
      "0 748 done\n",
      "0 749 done\n",
      "0 750 done\n",
      "0 751 done\n",
      "0 752 done\n",
      "0 753 done\n",
      "0 754 done\n",
      "0 755 done\n",
      "0 756 done\n",
      "0 757 done\n",
      "0 758 done\n",
      "0 759 done\n",
      "0 760 done\n",
      "0 761 done\n",
      "0 762 done\n",
      "0 763 done\n",
      "0 764 done\n",
      "0 765 done\n",
      "0 766 done\n",
      "0 767 done\n",
      "0 768 done\n",
      "0 769 done\n",
      "0 770 done\n",
      "0 771 done\n",
      "0 772 done\n",
      "0 773 done\n",
      "0 774 done\n",
      "0 775 done\n",
      "0 776 done\n",
      "0 777 done\n",
      "0 778 done\n",
      "0 779 done\n",
      "0 780 done\n",
      "0 781 done\n",
      "0 782 done\n",
      "0 783 done\n",
      "0 784 done\n",
      "0 785 done\n",
      "0 786 done\n",
      "0 787 done\n",
      "0 788 done\n",
      "0 789 done\n",
      "0 790 done\n",
      "0 791 done\n",
      "0 792 done\n",
      "0 793 done\n",
      "0 794 done\n",
      "0 795 done\n",
      "0 796 done\n",
      "0 797 done\n",
      "0 798 done\n",
      "0 799 done\n",
      "0 800 done\n",
      "0 801 done\n",
      "0 802 done\n",
      "0 803 done\n",
      "0 804 done\n",
      "0 805 done\n",
      "0 806 done\n",
      "0 807 done\n",
      "0 808 done\n",
      "0 809 done\n",
      "0 810 done\n",
      "0 811 done\n",
      "0 812 done\n",
      "0 813 done\n",
      "0 814 done\n",
      "0 815 done\n",
      "0 816 done\n",
      "0 817 done\n",
      "0 818 done\n",
      "0 819 done\n",
      "0 820 done\n",
      "0 821 done\n",
      "0 822 done\n",
      "0 823 done\n",
      "0 824 done\n",
      "0 825 done\n",
      "0 826 done\n",
      "0 827 done\n",
      "0 828 done\n",
      "0 829 done\n",
      "0 830 done\n",
      "0 831 done\n",
      "0 832 done\n",
      "0 833 done\n",
      "0 834 done\n",
      "0 835 done\n",
      "0 836 done\n",
      "0 837 done\n",
      "0 838 done\n",
      "0 839 done\n",
      "0 840 done\n",
      "0 841 done\n",
      "0 842 done\n",
      "0 843 done\n",
      "0 844 done\n",
      "0 845 done\n",
      "0 846 done\n",
      "0 847 done\n",
      "0 848 done\n",
      "0 849 done\n",
      "0 850 done\n",
      "0 851 done\n",
      "0 852 done\n",
      "0 853 done\n",
      "0 854 done\n",
      "0 855 done\n",
      "0 856 done\n",
      "0 857 done\n",
      "0 858 done\n",
      "0 859 done\n",
      "0 860 done\n",
      "0 861 done\n",
      "0 862 done\n",
      "0 863 done\n",
      "0 864 done\n",
      "0 865 done\n",
      "0 866 done\n",
      "0 867 done\n",
      "0 868 done\n",
      "0 869 done\n",
      "0 870 done\n",
      "0 871 done\n",
      "0 872 done\n",
      "0 873 done\n",
      "0 874 done\n",
      "0 875 done\n",
      "0 876 done\n",
      "0 877 done\n",
      "0 878 done\n",
      "0 879 done\n",
      "0 880 done\n",
      "0 881 done\n",
      "0 882 done\n",
      "0 883 done\n",
      "0 884 done\n",
      "0 885 done\n",
      "0 886 done\n",
      "0 887 done\n",
      "0 888 done\n",
      "0 889 done\n",
      "0 890 done\n",
      "0 891 done\n",
      "0 892 done\n",
      "0 893 done\n",
      "0 894 done\n",
      "0 895 done\n",
      "0 896 done\n",
      "0 897 done\n",
      "0 898 done\n",
      "0 899 done\n",
      "0 900 done\n",
      "0 901 done\n",
      "0 902 done\n",
      "0 903 done\n",
      "0 904 done\n",
      "0 905 done\n",
      "0 906 done\n",
      "0 907 done\n",
      "0 908 done\n",
      "0 909 done\n",
      "0 910 done\n",
      "0 911 done\n",
      "0 912 done\n",
      "0 913 done\n",
      "0 914 done\n",
      "0 915 done\n",
      "0 916 done\n",
      "0 917 done\n",
      "0 918 done\n",
      "0 919 done\n",
      "0 920 done\n",
      "0 921 done\n",
      "0 922 done\n",
      "0 923 done\n",
      "0 924 done\n",
      "0 925 done\n",
      "0 926 done\n",
      "0 927 done\n",
      "0 928 done\n",
      "0 929 done\n",
      "0 930 done\n",
      "0 931 done\n",
      "0 932 done\n",
      "0 933 done\n",
      "0 934 done\n",
      "0 935 done\n",
      "0 936 done\n",
      "0 937 done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[38;5;28mprint\u001b[39m(e,b, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# let's train, see how long it takes\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m train_network(network\u001b[38;5;241m=\u001b[39mfirst_model,\n\u001b[0;32m     35\u001b[0m              data_loader\u001b[38;5;241m=\u001b[39mdataloader,\n\u001b[0;32m     36\u001b[0m              epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Finished\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m, in \u001b[0;36mtrain_network\u001b[1;34m(network, data_loader, epochs, loss_function, learning_rate, optimizer)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# go through epochs\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e,epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# load a batch of training data for given epoch\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m b,batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[0;32m     20\u001b[0m         samples, labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m], batch[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     21\u001b[0m         \u001b[38;5;66;03m# reset gradient, need to calculate gradient afresh every time\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\whatever\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\whatever\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\whatever\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\whatever\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\whatever\\Lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\whatever\\Lib\\multiprocessing\\connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\whatever\\Lib\\multiprocessing\\connection.py:346\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    344\u001b[0m             _winapi\u001b[38;5;241m.\u001b[39mPeekNamedPipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(wait([\u001b[38;5;28mself\u001b[39m], timeout))\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\whatever\\Lib\\multiprocessing\\connection.py:896\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    893\u001b[0m                 ready_objects\u001b[38;5;241m.\u001b[39madd(o)\n\u001b[0;32m    894\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 896\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m _exhaustive_wait(waithandle_to_obj\u001b[38;5;241m.\u001b[39mkeys(), timeout)\n\u001b[0;32m    897\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    898\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32m~\\miniforge3\\envs\\whatever\\Lib\\multiprocessing\\connection.py:828\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    826\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m--> 828\u001b[0m     res \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mWaitForMultipleObjects(L, \u001b[38;5;28;01mFalse\u001b[39;00m, timeout)\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m    830\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "def train_network (network:nn.Module,\n",
    "                   data_loader:torch.utils.data.DataLoader,\n",
    "                   epochs:int,\n",
    "                   # these seem to be popular, beyond me why exactly\n",
    "                   loss_function:nn.Module = nn.CrossEntropyLoss(),\n",
    "                   # smaller so that model won't overfit\n",
    "                   learning_rate:float = 0.001,\n",
    "                   # here I found SDG and Adam as some popular options\n",
    "                   optimizer:torch.optim.Optimizer = torch.optim.Adam):\n",
    "    # set NN in training mode\n",
    "    network.train(True)\n",
    "\n",
    "    # set up optimizer\n",
    "    optimizer = optimizer(network.parameters(), lr = learning_rate)\n",
    "    # go through epochs\n",
    "    for e,epoch in enumerate(range(epochs)):\n",
    "        # load a batch of training data for given epoch\n",
    "        for b,batch in enumerate(data_loader):\n",
    "            samples, labels = batch[0], batch[1]\n",
    "            # reset gradient, need to calculate gradient afresh every time\n",
    "            optimizer.zero_grad()\n",
    "            # run input data through network (somehow this works without calling forward methods)\n",
    "            predictions = network(samples/255) # normalize to 0-1 todo: find better place to do this\n",
    "            # calculate eror\n",
    "            error = loss_function(predictions, labels)\n",
    "            # calculate gradient\n",
    "            error.backward()\n",
    "            # update weights by stepping down the gradient by learning rate sized step\n",
    "            optimizer.step()\n",
    "            print(e,b, 'done')\n",
    "\n",
    "# let's train, see how long it takes\n",
    "train_network(network=first_model,\n",
    "             data_loader=dataloader,\n",
    "             epochs = 10)\n",
    "\n",
    "print(\"Training Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1c028a-98f0-4ea3-a65a-f1da361bfbc8",
   "metadata": {},
   "source": [
    "TESTING\n",
    "evaluate performance ncorrect / nclassifications (for each digit & overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a3e6b-14d9-41c1-9433-a157bd2d8670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing loop\n",
    "def test_performance (loader:torch.utils.data.DataLoader,\n",
    "                      network:nn.Module):\n",
    "    # overall performance\n",
    "    ncorrect, nclassification = 0,0\n",
    "\n",
    "    # set model into testing mode\n",
    "    network.eval()\n",
    "    # context manager disables gradient calculation (only evaluation, doesn't change weights)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            outputs = network(images)\n",
    "            # get the index at which the maximum probability occured (reduce over columns = dimension 1{dimension 0 is rows})\n",
    "            predictions = torch.argmax(outputs,1)\n",
    "            # makes a mask of True & False, summing over true gives number correct since True = 1\n",
    "            ncorrect += sum(predictions == labels)\n",
    "            nclassification += len(predictions)\n",
    "                    \n",
    "    performance = float(ncorrect)/float(nclassification)\n",
    "    print(f'{ncorrect} / {nclassification} samples correct -> accuracy {performance*100:.2f} %')\n",
    "    return performance\n",
    "          \n",
    "# test on training data\n",
    "training_data_performance = test_performance(dataloader,\n",
    "                                             first_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633b7bf0-a881-4531-aa7a-165a72013ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on testing (unseen data)\n",
    "test_data_mnist = datasets.MNIST(root='.', # same directory\n",
    "                                download=True, # if already downloaded won't do anything\n",
    "                                transform = transform,\n",
    "                                # testing part of dataset\n",
    "                                train = False)\n",
    "\n",
    "testloader = DataLoader(dataset = test_data_mnist, \n",
    "                        batch_size = 64,\n",
    "                        num_workers = 4) # 4 cpu cores on my laptop\n",
    "\n",
    "testing_performance = test_performance(loader = testloader,\n",
    "                                      network = first_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ca1519-315c-41c9-8c07-62cc79386ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model with performance as part of name\n",
    "# change this to be current dir\n",
    "#path = r'C:\\Users\\matus\\OneDrive\\Documents\\university\\Psychology 3rd year\\Data Processing\\pytorch_final_project\\pytorch_final_project\\models'\n",
    "# everything we want about a model (weights and biases obtained through training)\n",
    "torch.save(first_model.state_dict(), '.' + f'\\models\\fully_connected_{testing_performance}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ff5f80-299d-414f-94fe-c171b50c6960",
   "metadata": {},
   "source": [
    "5 epochs: Not bad - performance on training and testing data almost the same ~ 67%\n",
    "\n",
    "10 epochs: Quite a bit better - performance on training and testing data ~ 87 % \n",
    "Goal should be to get this performance higher tho, to >90 % (most easily improved by adding convolutional layers?)\n",
    "\n",
    "changed model architecture from 300-200-300 hidden layer nodes to 200-300-200 and suddenly performance jumped to 98% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b73e0f-9aa5-4597-bbd6-9314daa72c65",
   "metadata": {},
   "source": [
    "USING HAND-DRAWN Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d101d68-d10e-4f67-aa6d-21ccca38c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to draw digits and label them\n",
    "%run draw.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac2390-02c1-410e-bb33-49ead7def2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for working with our images\n",
    "from PIL import Image, ImageOps\n",
    "from numpy import array, dstack\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms.functional import invert as tor_invert\n",
    "# filenames containing labels\n",
    "my_testing_set = os.listdir('drawing')\n",
    "# extract labels\n",
    "labels = [int(x[0]) for x in my_testing_set]\n",
    "# extract data\n",
    "data = []\n",
    "# transform data into tensor\n",
    "for sample in my_testing_set:\n",
    "    # get image\n",
    "    sample = ImageOps.grayscale(Image.open(f'drawing/{sample}'))\n",
    "    sample = sample.resize((28,28))\n",
    "    # turn into array\n",
    "    my_array = np.array(sample).astype('float32')\n",
    "    #print(my_array.shape)\n",
    "    data.append(my_array)\n",
    "# transform to tensor shape like before\n",
    "data = torch.from_numpy(\n",
    "    dstack(data).reshape((30,1,28,-1)))\n",
    "# invert colors (my drawings are black on white, MNIST are white on black)\n",
    "data = tor_invert(data\n",
    "                 )/255 # normalize to 0-1 values\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "# class for our dataset format so that I can use it for DataLoader\n",
    "class DrawDataset(Dataset):\n",
    "    def __init__(self, data:torch.tensor, labels:list):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "Drawings = DrawDataset(data,\n",
    "                       labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8617e3e9-d349-4bbb-9780-755d0f1587cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_loader = DataLoader(Drawings, \n",
    "                       batch_size = 4,\n",
    "                       num_workers = 0,\n",
    "                       shuffle = True)\n",
    "\n",
    "# load pretrained model so don't have to train every time [change to only do this if the model is there in the directory\n",
    "# and if so, use the one with highest accuracy (filename)\n",
    "fc_model = My_First_NN(784,\n",
    "                    # completely arbitrary number of hidden layers & nodes\n",
    "                    [200,300,200],\n",
    "                    10,\n",
    "                    nn.ReLU)\n",
    "\n",
    "models = os.listdir('models')\n",
    "# finds model with highest accuracy (always saved as last attribute\n",
    "model_name = np.argmax([float(m.split('_')[-1].split('.pt')[0]) for m in models])\n",
    "fc_model = fc_model.load_state_dict(torch.load('.' + f\"\\models\\{models[fc_model]}\")\n",
    "\n",
    "# handdrawn data performance\n",
    "hand_drawn_performance = test_performance(my_loader,\n",
    "                                         first_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aab45c-606e-40b3-9b43-19cf88e1c4b2",
   "metadata": {},
   "source": [
    "Performance on our own drawings is POOR (before inverting color it was 6%, now 10% still bad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
